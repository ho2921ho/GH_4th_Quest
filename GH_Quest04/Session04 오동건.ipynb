{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://news.naver.com/main/list.nhn?mode=LS2D&mid=shm&sid1=100&sid2=269\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html,\"lxml\",from_encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = soup.findAll(\"dt\")\n",
    "ex_dl = articles[1].a[\"href\"] # .a 메서드를 사용해서 a태그 안의 href 속성을 볼러온다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "\n",
    "for link in articles:\n",
    "    url_list.append(link.find(\"a\")[\"href\"])\n",
    "\n",
    "url_list = list(set(url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=100&sid2=269&oid=003&aid=0008832790'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list[0]\n",
    "html = urlopen(url_list[0])\n",
    "soup = BeautifulSoup(html,\"html.parser\", from_encoding = \"utf-8\")\n",
    "content = soup.find(id = 'articleBodyContents')# id로 content를 포함하는 고유한 값을 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_url = \"https://news.naver.com/main/list.nhn?mode=LS2D&sid2=269&sid1=100&mid=shm&date=20181001&page=\"\n",
    "\n",
    "for i in range(1,6):\n",
    "    page_url = url+str(i)\n",
    "    page_html = urlopen(page_url)\n",
    "    page_soup = BeautifulSoup(page_html,\"lxml\",from_encoding = \"utf-8\")\n",
    "    articles = page_soup.findAll(\"dt\")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스 기사 페이지에서 제목과 내용을 가져오는 함수를 정의한다. \n",
    "def NewsCraw(url):\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html,\"html.parser\", from_encoding = \"utf-8\")\n",
    "    content = soup.find(id = 'articleBodyContents')# id로 content를 포함하는 고유한 값을 불러온다.\n",
    "    content = str(content) # html 을 string으로 바꾼다. \n",
    "    p = re.compile(\"<br/>.*[.]<br/><br/>\") # 본문 내용만 뽑기 위해 정규표현식을 만든다.\n",
    "    content = p.findall(content) # 정규표현식에 해당하는 부분을 가져온다.\n",
    "    content = re.sub(\"<br/>\",\"\",content) # <br/>부분을 지운다. \n",
    "    title = soup.find(\"h3\",{\"id\":\"articleTitle\"}).get_text() # 기사 제목을 텍스트로 가져온다.\n",
    "    return [title, content]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
