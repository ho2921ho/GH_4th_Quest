{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈을 불러온다. \n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import konlpy\n",
    "import nltk\n",
    "from konlpy.tag import Kkma;kkma = Kkma()\n",
    "from konlpy.corpus import kobill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------NewsCrawling------------------#\n",
    "# 특정 기사의 본문과 제목을 가져오는 함수. \n",
    "def NewsCraw(url):\n",
    "    html = urlopen(url) # url에서 html을 가져온다. \n",
    "    soup = BeautifulSoup(html,\"html.parser\", from_encoding = \"utf-8\") # bs4에서 파싱할 수 있는 구조로 바꾼다.\n",
    "    content = soup.find(id = 'articleBodyContents') # id로 본문 내용을 가져온다. \n",
    "    hangul = re.compile(\"[^ ㄱ-ㅎ|가-힣.\"\"]+\") # 한글,.,띄어쓰기가 아닌 문자(영어, 특수문자, 숫자 등등)를 나타내는 정규표현식을 만든다.\n",
    "    content = str(content) # html 객체?를 문자열로 바꾼다.\n",
    "    content = hangul.sub(\"\",content) # 정규표현식에 해당하는 패턴을 지운다. \n",
    "    content = re.sub(\"본문 내용  플레이어   플레이어    오류를 우회하기 위한 함수 추가\",\"\",content) # 불필요한 한글 패턴 제거\n",
    "    content = re.sub('본문 내용  플레이어   동영상 뉴스       영상 플레이어   플레이어    오류를 우회하기 위한 함수 추가','',content)\n",
    "    content = re.sub(\"본문 내용  플레이어   동영상 뉴스 .      영상 플레이어   플레이어    오류를 우회하기 위한 함수 추가\",'',content)\n",
    "    title = soup.find(\"h3\",{\"id\":\"articleTitle\"}).find(text=True) # 제목을 가져온다. \n",
    "    return [title ,content] # 제목과 내용을 반환한다. \n",
    "\n",
    "# 기사목록에서 개별기사를 가져오는 함수.\n",
    "def articles(url):\n",
    "    articles = []\n",
    "    for i in range(1,6):\n",
    "        page_url = url+str(i) # i를 문자열로 변경.\n",
    "        page_html = urlopen(page_url) \n",
    "        page_soup = BeautifulSoup(page_html,\"html.parser\",from_encoding = \"utf-8\")\n",
    "        articles.extend(page_soup.findAll(\"dt\"))\n",
    "        url_list = []\n",
    "        for link in articles:\n",
    "            url_list.append(link.find(\"a\")[\"href\"])\n",
    "            url_list = list(set(url_list))\n",
    "    return url_list\n",
    "\n",
    "# 기사에서 제목과 내용을 가져오는 함수.\n",
    "def ticon(url_list):\n",
    "    df = DataFrame(columns=(\"기사제목\",\"본문내용\"))\n",
    "    for i, url in enumerate(url_list):\n",
    "        df.loc[i] = NewsCraw(url)\n",
    "    return df\n",
    "\n",
    "# 뉴스 주제별로 기사목록을 가져온다. \n",
    "pol_url = \"https://news.naver.com/main/list.nhn?mode=LS2D&sid2=269&sid1=100&mid=shm&date=20181010&page=\" \n",
    "cul_url = \"https://news.naver.com/main/list.nhn?mode=LS2D&sid2=245&sid1=103&mid=shm&date=20181010&page=\"\n",
    "sci_url = \"https://news.naver.com/main/list.nhn?mode=LS2D&sid2=228&sid1=105&mid=shm&date=20181010&page=\"\n",
    "\n",
    "# 토픽에 따라 기사들을 정리. \n",
    "pol_articles = articles(pol_url)\n",
    "cul_articles = articles(cul_url)\n",
    "sci_articles = articles(sci_url)\n",
    "\n",
    "pol_articles = ticon(pol_articles)\n",
    "cul_articles = ticon(cul_articles)\n",
    "sci_articles = ticon(sci_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------LDA------------------#\n",
    "# Tokenize\n",
    "pol_doc = []\n",
    "for i in range(0,len(pol_articles)):\n",
    "    pol_doc.append(kkma.morphs(pol_articles.iloc[i,1]))\n",
    "\n",
    "cul_doc = []\n",
    "for i in range(0,len(cul_articles)):\n",
    "    cul_doc.append(kkma.morphs(cul_articles.iloc[i,1]))\n",
    "\n",
    "sci_doc = []\n",
    "for i in range(0,len(sci_articles)):\n",
    "    sci_doc.append(kkma.morphs(sci_articles.iloc[i,1]))\n",
    "\n",
    "len(pol_doc) #100\n",
    "len(cul_doc) #100\n",
    "len(sci_doc) #81\n",
    "\n",
    "# LDA\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "documents = []\n",
    "documents.extend(pol_doc)\n",
    "documents.extend(cul_doc)\n",
    "documents.extend(sci_doc)\n",
    "len(documents) # 1~100까지는 정치 101~200까지는 문화 200~281은 과학"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#조건부 확률 분포 정의를 위한 준비\n",
    "\n",
    "#topic의 개수\n",
    "K = 3\n",
    "\n",
    "#1. 각 토픽이 각 문서에 할당되는 횟수\n",
    "#counter로 구성된 list\n",
    "#각각의 counter는 각 문서를 의미함\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "\n",
    "#2. 각 단어가 각 토픽에 할당되는 횟수\n",
    "# 각각의 counter는 각 토픽을 의미함\n",
    "topic_word_counts = [Counter() for _ in range(K)] \n",
    "\n",
    "#3. 각 토픽에 할당되는 총 단어 수\n",
    "# 각각의 숫자는 각 토픽을 의미함\n",
    "topic_counts = [0 for _ in range(K)] \n",
    "\n",
    "#4. 각 문서에 포함되는 총 단어의 수\n",
    "# 각각의 숫자는 각 문서를 의미함\n",
    "document_lengths = [len(d) for d in documents]\n",
    "\n",
    "#5. 단어 종류의 수\n",
    "distinct_words = set(word for document in documents for word in document) \n",
    "W = len(distinct_words)\n",
    "\n",
    "#6. 총 문서의 수 \n",
    "D = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 새로운 topic 계산하기\n",
    "\n",
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    # 문서 d의 모든 단어 가운데 topic에 속하는\n",
    "    # 단어의 비율 (alpha를 더해 smoothing)\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    # topic에 속한 단어 가운데 word의 비율\n",
    "    # (beta를 더해 smoothing)\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + W * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    # 문서와 문서의 단어가 주어지면\n",
    "    # k번째 토픽의 weight를 반환\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "#랜덤으로 생성된 weight로부터 인덱스를 생성함\n",
    "def sample_from(weights):\n",
    "     total = sum(weights)\n",
    "     rnd = total * random.random()       # uniform between 0 and total\n",
    "     for i, w in enumerate(weights):\n",
    "         rnd -= w                        # return the smallest i such that\n",
    "         if rnd <= 0: return i           # sum(weights[:(i+1)]) >= rnd\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "#topic의 개수\n",
    "K = 3\n",
    "\n",
    "# 각 단어를 임의의 토픽에 배정\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in documents]\n",
    "\n",
    "# 랜덤 초기화한 상태에서 AB를 구하는 데 필요한 숫자 계산하기\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "\n",
    "# 조건부 확률 분포를 이용하여 (토픽-단어), (문서-토픽)에 대한 깁스 샘플링 실행하기\n",
    "for iter in range(1000): \n",
    "    print(iter)\n",
    "    for d in range(D): \n",
    "        for i, (word, topic) in enumerate(zip(documents[d], \n",
    "                                              document_topics[d])): \n",
    " \n",
    " \n",
    "           # remove this word / topic from the counts\n",
    "           # so that it doesn't influence the weights \n",
    "            document_topic_counts[d][topic] -= 1 \n",
    "            topic_word_counts[topic][word] -= 1 \n",
    "            topic_counts[topic] -= 1 \n",
    "            document_lengths[d] -= 1 \n",
    " \n",
    "           # choose a new topic based on the weights \n",
    "            new_topic = choose_new_topic(d, word) \n",
    "            document_topics[d][i] = new_topic \n",
    "\n",
    " \n",
    "           # and now add it back to the counts \n",
    "            document_topic_counts[d][new_topic] += 1 \n",
    "            topic_word_counts[new_topic][word] += 1 \n",
    "            topic_counts[new_topic] += 1 \n",
    "            document_lengths[d] += 1\n",
    "\n",
    "#각 토픽에 가장 영향력이 높은 (weight)값이 큰 단어 탐색\n",
    "for k, word_counts in enumerate(topic_word_counts): \n",
    "         for word, count in word_counts.most_common(): \n",
    "             if count > 0: print (k, word, count) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'정치': 87, '과학': 11, '문화': 2})\n",
      "Counter({'문화': 72, '과학': 18, '정치': 10})\n",
      "Counter({'과학': 70, '정치': 9, '문화': 1})\n"
     ]
    }
   ],
   "source": [
    "topic_names = [\"문화\", \"정치\", \"과학\"]\n",
    "\n",
    "df = DataFrame(columns=(\"토픽\",\"횟수\",\"문서번호\"))\n",
    "num = -1 \n",
    "for document, topic_counts in zip(documents, document_topic_counts): \n",
    "        num += 1\n",
    "        for topic, count in topic_counts.most_common():\n",
    "            temp = pd.DataFrame({\"토픽\":topic_names[topic],\"횟수\":count,\"문서번호\":num}, index = [0])\n",
    "            df = df.append(temp)\n",
    "\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "pred_topics = []\n",
    "for i,_ in enumerate(df.T):\n",
    "    if i%3 == 0:\n",
    "        df2 = df[i:i+3]\n",
    "        topic = df2[\"토픽\"][df2[\"횟수\"].astype('float64').idxmax()] # 실수로 바꺼야 idmax가 먹힌다...\n",
    "        pred_topics.append(topic)\n",
    "\n",
    "len(pred_topics) #281\n",
    "\n",
    "print(Counter(pred_topics[0:100])) # 실제 정치 기사\n",
    "print(Counter(pred_topics[100:200]))# 실제 문화 기사\n",
    "print(Counter(pred_topics[200:280])) # 실제 과학 기사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counter({'정치': 87, '과학': 11, '문화': 2}) <br>\n",
    "Counter({'문화': 72, '과학': 18, '정치': 10}) <br>\n",
    "Counter({'과학': 70, '정치': 9, '문화': 1}) <br>\n",
    "\n",
    "LDA로 문서를 잘 구분한다. 특히 과학기사를 잘 구분한다. 아무래도 정치/문화 보다 전문적인 용어가 많고 기사가 서로 겹치지 않아서 그런 듯 하다. <br> \n",
    "하지만 몇몇 기사는 잘 구분하지 못하는 데, 세션별로 기사가 겹치는 경우가 있기 때문으로 생각된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------Word2Vec------------------#\n",
    "# Word2Vec 모델 만들기\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(documents,size=500, window=10, min_count=5, sg=1) # 여러번 해보니깐 모수 조절이 중요한듯? 어떤 기준으로 정해야할까..\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "print(model.similarity('뮤지컬', '영화'))\n",
    "print(model.similarity('여당', '대통령'))\n",
    "print(model.similarity('연구', '과학'))\n",
    "print(model.similarity('남북', '뮤지컬'))\n",
    "print(model.similarity('민주당', '축제'))\n",
    "\n",
    "print(model.most_similar(\"북한\")) # 파라미터들을 코드 대로 size=100, window=3, min_count=2, sg=1 했을 때는 최빈값이 부동산..이었음.. 파라미터를 조절하니깐 합리적으로 나온다! 파라미터 조절을 어떻게 해야할까..\n",
    "\n",
    "print(model.most_similar(positive=['북한', '비핵화'], negative='야당', topn=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
